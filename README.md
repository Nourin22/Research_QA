# ğŸ§  ResearchQA: Research Paper Question-Answering System (RAG + Ollama)

This project is a **Research Paper Question-Answering System** built with the Retrieval-Augmented Generation (RAG) approach using **LangChain**, **Ollama** (for local LLMs), **ChromaDB**, and **Streamlit**. It enables researchers to **ask natural language questions** over uploaded research papers (PDF format), and get concise answers powered by a local LLM like `mistral`.

---

## ğŸš€ Features

- ğŸ“„ Upload and process research papers in PDF format
- ğŸ” Uses semantic search via `ChromaDB` vector store
- ğŸ§  Integrates with `mistral` LLM running locally via **Ollama**
- ğŸ¦œ Built using LangChain's RAG pipeline
- ğŸ’¬ Intuitive UI built with Streamlit

---

## ğŸ–¼ï¸ Architecture

[PDF Research Paper] â†’ [LangChain PDF Loader]
â†“
[Text Splitting]
â†“
[Embeddings via SentenceTransformer]
â†“
[ChromaDB Vector Store (FAISS-like)]
â†“
[User Question] â†’ [LangChain RetrievalQA Chain] â†’ [Ollama LLM (mistral)] â†’ [Answer]


---

## ğŸ› ï¸ Installation

1. Create & Activate Virtual Environment

python -m venv venv
venv\Scripts\activate      # On Windows

2. Install Dependencies
3. 
pip install -r requirements.txt

5. Install and Run Ollama
Install Ollama (if not installed):

ğŸ”— https://ollama.com/download

Start Ollama and pull the mistral model:
ollama pull mistral
ollama run mistral
ğŸ§ª Running the App

streamlit run app.py
Upload your research paper (PDF), ask a question, and get contextual answers!


ğŸ§  Models Used
Sentence Transformers: all-MiniLM-L6-v2 for generating embeddings

LLM: mistral via Ollama

ğŸ§© External Tools Used
LangChain

Ollama â€“ Local LLMs like mistral

ChromaDB â€“ Lightweight vector store

Streamlit â€“ Interactive UI

ğŸ“Œ Notes
Ensure ollama server is running before starting the app.

Make sure the mistral model is pulled (ollama pull mistral).
